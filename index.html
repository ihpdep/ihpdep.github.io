<!DOCTYPE html>

<html>
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <meta name="keywords" content="Yang Li">
        <meta name="description" content="Academic homepage of Yang Li">
        <meta name="viewport" content="width=device-wdith; initial-scale=1.0;">
        <title>Yang Li's personal homepage</title>
        <link href="css/main.css" rel="stylesheet">
        <link href="css/bootstrap.min.css" rel="stylesheet">
        <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,300' rel='stylesheet'>
	      <base target="_blank">
    </head>
    <body>
        <div class="container">
            <header class="row">
                <div class="myinfo col-8 col-sm-6 col-xs-12">
                    <h1>Yang Li, Ph.D.  </h1>
                    <h1>李&nbsp;&nbsp;洋</h1>
                    <!-- <p>
                        <a href="http://www.cs.zju.edu.cn/english/">College of Computer Science</a>
                    </p>
                    <p>
                        <a href="http://www.zju.edu.cn/english/">Zhejiang University</a>
                    </p> -->
                    <p>
                        Address: RM 101, Zetong Building,
                    </p>
                    <p>
                        Yuquan Campus, Hangzhou,China
                    </p>
                    <p>
                        Email: liyang89 AT zju DOT edu DOT cn
                    </p>
                   				            <p>[<a href="https://github.com/ihpdep/ihpdep.github.io/raw/master/files/cv-liyang.pdf">Curriculum Vitae</a>][<a href="https://scholar.google.com/citations?user=N1ZDSHYAAAAJ">Google Scholar</a>]</p>
                </div>
                <div class="col-4 col-sm-6 col-xs-12">
                    <img class="portrait img-responsive" src="files/photo-new.jpg">
                </div>
            </header>
            <div class="row">
                <div class="col-12 col-sm-12 col-xs-12">
                    <hr>
                </div>
            </div>
            <main>
                <div class="row">
                    <div class="col-12 col-sm-12 col-xs-12">
                        <h3>About Me</h3>
                        <p>
                            I am going to join the <a href="http://www.cs.ecnu.edu.cn/">School of Computer Science and Technology</a> at <a href="http://english.ecnu.edu.cn/">East China Normal University</a> as  an associate professor this October.
                            I completed my PhD in the College of Computer Science at Zhejiang University with my advisor Prof. <a href="https://person.zju.edu.cn/en/jkzhu">Jianke Zhu</a>. In 2018, I visited the <a href="https://ucsd.edu">University of California, San Diego</a> and worked with Prof. <a href="https://yip.eng.ucsd.edu/">Michael Yip</a> in ECE department. Also, I had worked as a part-time researcher at <a href="https://azft.alibaba.com/">Alibaba-Zhejiang University Joint Institute of Frontier Technologies</a> during my PhD. Prior to that, I spent some years in video game industries at <a href="https://virtuosgames.com/en">Virtuos</a>. 
                        </p>
                        <p style="color:red;">
                            I am currently looking for MSc students in 2021. If you are interested in my research work, please send your CV and research proposal to liyang89 AT zju DOT edu DOT cn.
                        </p>
                    </div>
                </div>
                <div class="row">
                    <div class="col-12 col-sm-12 col-xs-12">
                        <hr>
                    </div>
                </div>
                <div class="row">
                    <div class="col-12 col-sm-12 col-xs-12">
                        <h3>Research Interest</h3>
                        <p>
                            My research interests are in the areas of Computer Vision and Machine Learning and, particularly, in methods of extracting meaningful information and structural data from videos and raw streaming sources.
                            I am also very interested in finding and defining temporal information between video frames to provide visual perception for cross-disciplinary researches.
                            Currently, my focus is on model-free single object tracking and 3D reconstruction/fusion.
                        </p>

                    </div>
                </div>
                <div class="row">
                    <div class="col-12 col-sm-12 col-xs-12">
                        <hr>
                    </div>
                </div>
                <div class="row">
                    <div class="col-12 col-sm-12 col-xs-12">
                        <h3>Selected Research Projects</h3>
                        <div class="custom-container container">
                            <div class="col-4 col-sm-4 col-xs-6"><img class="portrait img-responsive" src="files/example.gif"></div>
                            <div class="desc col-8 col-sm-8 col-xs-6">
                                <h4><b>Visual Object Tracking</b></h4> 
                                <p>Given a video sequence and a selected target in the first frame, visual object tracking aims to track the target object robustly and accurately during the whole video sequence. We are interested in different geometric representations of the tracking algorithm. With various geometric representations, visual object tracking methods can be viewed as basic blocks for many high-level applications, such as surveillance, video editing/analysis, etc. </p>   
                            </div>
                        </div>

                        <div class="custom-container container">
                            <div class="col-4 col-sm-4 col-xs-6"><img class="portrait img-responsive" src="files/3dreconstruction.gif"></div>
                            <div class="desc col-8 col-sm-8 col-xs-6">
                                <h4><b>3D Motion Capture</b></h4> 
                                <p>Taking one step forward, estimating the geometric representation of a visual object in the RGB-D sequence leads us to the 3D motion capture topic. It is highly related to 3D dynamic reconstruction, RGB-D fusion and 3D visual tracking. The algorithm outputs all geometric properties for every pixel in the sequence. It can be viewed as a fundamental 3D perception method which is very useful in AR/VR and robotics. The 3D reconstruction technique can also be applied to game/film making, geography, and so on.  </p>   
                            </div>
                        </div>

                        <div class="custom-container container">
                            <div class="col-4 col-sm-4 col-xs-6"><img class="portrait img-responsive" src="files/superfull.gif"></div>
                            <div class="desc col-8 col-sm-8 col-xs-6">
                                <h4><b>Interdisciplinary Projects</b></h4> 
                                <p>With the capabilities of perceiving visual information in a sequence/video stream, we have succeeded in applying computer vision algorithms to surgical robots to automatically manipulate bio-tissue. In the future, we are planning to continue exploring the possibilities of computer vision-related interdisciplinary projects. </p>   
                            </div>
                        </div>
                    </div>
                </div>
                <div class="row">
                    <div class="col-12 col-sm-12 col-xs-12">
                        <hr>
                    </div>
                </div>
                <div class="row">
                    <div class="col-12 col-sm-12 col-xs-12">
                        <h3>Publications</h3>
                    </div>
                </div>
                <div class="row">
                    <div class="col-12 col-sm-12 col-xs-12">
                        <p class="pub">
                            <b>SuPer Deep: A Surgical Perception Framework for Robotic Tissue Manipulation using Deep Learning for Feature Extraction</b> <br>
                            Jingpei Lu, Ambareesh Jayakumari, Florian Richter, Yang Li and Michael C. Yip<br>
                            <i>Arxiv</i>, 2020<br>
                            [<a href="https://arxiv.org/abs/2003.03472">PDF</a>]
			                      [<a href="https://sites.google.com/ucsd.edu/super-framework">Project Webpage</a>]
                        </p>
                    </div>
                </div>
                <div class="row">
                    <div class="col-12 col-sm-12 col-xs-12">
                        <p class="pub">
                            <b>Attribute-aware Pedestrian Detection in a Crowd</b> <br>
                            Jialiang Zhang, Lixiang Lin, Jianke Zhu, Yang Li, Yun-chen Chen, Yao Hu, Steven C.H. Hoi<br>
                            <i>IEEE Trans. on Multimedia</i>, 2020<br>
                            [<a href="https://arxiv.org/abs/1910.09188">PDF</a>]
			                      [<a href="https://github.com/kalyo-zjl/APD">CODE</a>]
                        </p>
                    </div>
                </div>
		        <div class="row">
                    <div class="col-12 col-sm-12 col-xs-12">
                        <p class="pub">
                            <b>SuPer: A Surgical Perception Framework for Endoscopic Tissue Manipulation with Surgical Robotics</b> <br>
                            Yang Li, Florian Richter, Jingpei Lu, Emily K. Funk, Ryan K. Orosco, Jianke Zhu and Michael C. Yip<br>
                            <i>IEEE Robotics and Automation Letters</i>, 2020<br>
                            [<a href="https://arxiv.org/abs/1909.05405">PDF</a>]
			                      [<a href="https://sites.google.com/ucsd.edu/super-framework">Project Webpage</a>]
                        </p>
                    </div>
                </div>
                <div class="row">
                    <div class="col-12 col-sm-12 col-xs-12">
                        <p class="pub">
                            <b>DeepFacade: A deep learning approach to facade parsing with symmetric loss</b> <br>
                            Hantang Liu, Yinghao Xu, Jialiang Zhang, Jianke Zhu, Yang Li, Steve C.H. Hoi<br>
                            <i>IEEE Trans. on Multimedia</i>, 2020<br>
			    [<a href="https://ieeexplore.ieee.org/document/8979370">PDF</a>]
                        </p>
                    </div>
                </div>
		            <div class="row">
                    <div class="col-12 col-sm-12 col-xs-12">
                        <p class="pub">
                            <b>Robust Estimation of Similarity Transformation for Visual Object Tracking</b> <br>
                            Yang Li, Jianke Zhu, Steven C.H. Hoi, Wenjie Song, Zhefeng Wang, Hantang Liu<br>
                            <i>The Conference on Association for the Advancement of Artificial Intelligence (AAAI)</i>, 2019<br>
                            [<a href="https://arxiv.org/abs/1712.05231">PDF</a>]
			                      [<a href="https://github.com/ihpdep/LDES">CODE</a>]
                            [<a href="https://sites.google.com/view/ldestracker">Project Webpage</a>]
                        </p>
                    </div>
                </div>
		            <div class="row">
                    <div class="col-12 col-sm-12 col-xs-12">
                        <p class="pub">
                            <b>Temporally-Adjusted Correlation Filter-based Tracking</b> <br>
                            Wenjie Song, Yang Li, Jianke Zhu, Chun Chen<br>
                            <i>Neurocomputing</i>, 2018<br>
                            [<a href="https://www.sciencedirect.com/science/article/pii/S0925231218301000">PDF</a>]
                        </p>
                    </div>
                </div>
                <div class="row">
                    <div class="col-12 col-sm-12 col-xs-12">
                        <p class="pub">
                            <b>CFNN: Correlation Filter Neural Network for Visual Object Tracking</b> <br>
                            Yang Li, Zhan Xu and Jianke Zhu<br>

                            <i>International Joint Conference on Artificial Intelligence</i> (IJCAI), 2017<br>
                            [<a href="https://github.com/ihpdep/ihpdep.github.io/raw/master/papers/ijcai17_cfnn.pdf">PDF</a>]
			                      [<a href="https://github.com/enderhsu/CFNN">CODE</a>]
                        </p>
                    </div>
                </div>
                <div class="row">
                    <div class="col-12 col-sm-12 col-xs-12">
                        <p class="pub">
                            <b>Reliable Patch Trackers: Robust Visual Tracking by Exploiting Reliable Patches</b> <br>
                            Yang Li, Jianke Zhu, Steven C.H. Hoi<br>

                            <i>Computer Vision and Pattern Recognition</i> (CVPR), 2015<br>
                            [<a href="https://github.com/ihpdep/ihpdep.github.io/raw/master/papers/cvpr15_rpt.pdf">PDF</a>]
                            [<a href="https://github.com/ihpdep/ihpdep.github.io/raw/master/papers/cvpr15_rpt_ext.pdf">ABSTRACT</a>]
                            [<a href="https://github.com/ihpdep/rpt">CODE</a>]
                        </p>
                    </div>
                </div>
                <div class="row">
                    <div class="col-12 col-sm-12 col-xs-12">
                        <p class="pub">
                            <b>Image Alignment by Online Robust PCA via Stochastic Gradient Descent</b><br>
                            Wenjie Song, Jianke Zhu,  Yang Li, Chun Chen.   <br>
                            <i>IEEE Transactions on Circuits and Systems for Video Technology</i> (TCSVT), 2015<br>
                            [<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7155543">PDF</a>]
                        </p>
                    </div>
                </div>
                <div class="row">
                    <div class="col-12 col-sm-12 col-xs-12">
                        <p class="pub">
                            <b>A Scale Adaptive Kernel Correlation Filter Tracker with Feature Integration </b><br>
                            Yang Li, Jianke Zhu<br>

                            <i>European Conference on Computer Vision, Workshop VOT2014</i> (ECCVW), 2014.<br>
                            [<a href="https://github.com/ihpdep/ihpdep.github.io/raw/master/papers/eccvw14_samf.pdf">PDF</a>]
                            [<a href="https://github.com/ihpdep/samf">CODE</a>]
                        </p>
                    </div>
                </div>
                <div class="row">
                    <div class="col-12 col-sm-12 col-xs-12">
                        <p class="pub">
                            <b>The visual object tracking vot2014 challenge results. </b><br>
                            M. Kristan, R. Pflugfelder, et al. (Co-author)<br>
                            <i>In ECCV 2014 Workshops, Workshop on Visual Object Tracking Challenge</i> 2014 <br>
                            [<a href="http://www.epics-project.eu/publications/2014_kristan_iccvw.pdf">PDF</a>]
                            We won the <i>second place</i> in <a href="http://votchallenge.net/vot2014/index.html">VOT2014 challenge</a>.</p>
                    </div>
                </div>
                <div class="row">
                    <div class="col-12 col-sm-12 col-xs-12">
                        <p class="pub">
                            <b>The visual object tracking vot2013 challenge results. </b><br>
                            M. Kristan, R. Pflugfelder, et al. (Co-author)<br>
                            <i>In ICCV 2013 Workshops, Workshop on Visual Object Tracking Challenge</i> 2013 <br>
                            [<a href="http://personal.ee.surrey.ac.uk/Personal/R.Bowden/publications/2013/Kristan_VOT_2013_ICCV_paper.pdf">PDF</a>]
                        </p>
                    </div>
                </div>
                <div class="row">
                    <div class="col-12 col-sm-12 col-xs-12">
                        <p class="pub">
                            <b>Adaptive lattice-based light rendering of participating media. </b><br>
                            Changbo Wang, Chenhui Li, Jinqiu Dai, Yang Li <br>
                            <i>Journal of Computer Animation and Virtual World</i> 22(6): 487-498 (2011). <br>
                            [<a href="http://onlinelibrary.wiley.com/doi/10.1002/cav.426/pdf">PDF</a>]
                        </p>
                    </div>
                </div>
                <div class="row">
                    <div class="col-12 col-sm-12 col-xs-12">
                        <p class="pub">
                            <b>Real-time realistic rendering of under seawater scene. </b><br>
                            Chenhui Li, Changbo Wang, Yang Li, Min Zhao, et al.<br>
                            <i>Journal of Image and Graphics.</i> 2011.16(8):1497-1502.<br>
                            [<a href="http://en.cnki.com.cn/Article_en/CJFDTotal-ZGTB201108022.htm">PDF</a>]
                        </p>
                    </div>
                </div>
                <div class="row">
                    <div class="col-12 col-sm-12 col-xs-12">
                        <hr>
                    </div>
                </div>

                
                <center>
                    <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=tt&d=U_jew6EMXsGBevNcBBxwLKEy57jIMni4L31hdsHL3Yg'></script>

                    <a href='https://clustrmaps.com/site/h7fl'  title='Visit tracker'><img src='//clustrmaps.com/map_v2.png?cl=ffffff&w=a&t=tt&d=U_jew6EMXsGBevNcBBxwLKEy57jIMni4L31hdsHL3Yg'/></a>

            </center>

        </main>
    </div>
    <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=U_jew6EMXsGBevNcBBxwLKEy57jIMni4L31hdsHL3Yg&cl=ffffff&w=a"></script>

    <script type="text/javascript" id="clustrmaps" src="https//clustrmaps.com/map_v2.js?d=U_jew6EMXsGBevNcBBxwLKEy57jIMni4L31hdsHL3Yg&cl=ffffff&w=a"></script>

</body></html>
